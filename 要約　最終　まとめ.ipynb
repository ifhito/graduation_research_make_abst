{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "import nagisa\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import MeCab\n",
    "import gensim\n",
    "from scipy import spatial\n",
    "import re\n",
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import requests\n",
    "from scipy.stats import norm, entropy\n",
    "from collections import namedtuple\n",
    "import glob\n",
    "import CaboCha\n",
    "import jctconv, os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pykakasi import kakasi\n",
    "import unicodedata\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイルの取得\n",
    "\n",
    "pass1=glob.glob(\"../text/*\")#ここのパスは個々人で書き換え\n",
    "pass2=glob.glob(pass1[0]+\"/*\")\n",
    "\n",
    "texts=[]\n",
    "texts_title=[]\n",
    "texts_original=[]\n",
    "for i in pass1:\n",
    "    pass2=glob.glob(i+\"/*\")\n",
    "    for i1 in pass2:\n",
    "        with open(i1) as f:\n",
    "            areas = f.readlines() \n",
    "            #いらない部分を削除\n",
    "        areas.pop(0)\n",
    "        areas.pop(0)\n",
    "        #タイトルの空白は削除\n",
    "        ls=\"\".join(areas[0].split())\n",
    "        #テキストタイトルの取得\n",
    "        texts_title.append(ls)\n",
    "        #タイトルの削除\n",
    "        areas.pop(0)\n",
    "        areas = ''.join(areas)\n",
    "        texts.append(areas)\n",
    "        texts_original.append(areas)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テキストの空白削除\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    texts[i]=\"\".join(texts[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用する文章の作成(get_sentencesで区切る)\n",
    "\n",
    "POS_DIC = {\n",
    "    'BOS/EOS': 'EOS', # end of sentense\n",
    "    '形容詞' : 'ADJ',\n",
    "    '連体詞' : 'JADJ', # Japanese-specific POS like a adjective\n",
    "    '副詞'   : 'ADV',\n",
    "    '名詞'   : 'NOUN',\n",
    "    '動詞'   : 'VERB',\n",
    "    '助動詞' : 'AUX',\n",
    "    '助詞'   : 'PART',\n",
    "    '感動詞' : 'INTJ',\n",
    "    '接続詞' : 'CONJ',\n",
    "    '記号'   : 'SYM', # symbol\n",
    "    '*'      : 'X',\n",
    "    'その他' : 'X',\n",
    "    'フィラー': 'X',\n",
    "    '接頭詞' : 'X',\n",
    "}\n",
    "POS_2ND_DIC = {\n",
    "    '代名詞':'PRON',\n",
    "}\n",
    "\n",
    "def get_sentences(text):\n",
    "    \"\"\" \n",
    "    input: text have many sentences\n",
    "    output: ary of sentences ['sent1', 'sent2', ...]\n",
    "    \"\"\"\n",
    "    EOS_DIC = ['。', '．', '！','？','!?', '!', '?' ]\n",
    "    sentences = list()#からのリスト\n",
    "    sent = ''#空のリスト\n",
    "    # split in first when text include '\\n'\n",
    "    temp = text.split('\\\\n')#tempにテキストを空白文字で区切ったもの\n",
    "    for each_text in temp:#tempでfor\n",
    "        if each_text == '':#からならば\n",
    "            continue\n",
    "        for token in tokenize(each_text):\n",
    "            # print(token.pos_jp, token.pos, token.surface, sent)\n",
    "            # TODO: this is simple way. ex)「今日は雨ね。」と母がいった\n",
    "            sent += token.surface#sentに表層形を足す\n",
    "            if token.surface in EOS_DIC and sent != '':#EOS_DICの中にtoken.surfaceがあり、sentがからでなければ\n",
    "                sentences.append(sent)#sentencesにsentをappend\n",
    "                sent = ''#sentを空に\n",
    "        if sent != '':#sentがからでなければ\n",
    "            sentences.append(sent)#sentencesにsentをappend\n",
    "    return sentences\n",
    "\n",
    "def tokenize(sent):\n",
    "    tagger = MeCab.Tagger ('-Ochasen /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "    tagger.parse('') \n",
    "    node = tagger.parseToNode( sent )\n",
    "    #print(sent)\n",
    "    tokens = list()\n",
    "    idx = 0\n",
    "    while node:\n",
    "        #print(node.surface)\n",
    "        feature = node.feature.split(',')\n",
    "        token = namedtuple('Token', 'idx, surface, pos, pos_detail1, pos_detail2, pos_detail3,infl_type, infl_form, base_form, reading, phonetic')\n",
    "        token.idx         = idx\n",
    "        token.surface     = node.surface  # 表層形\n",
    "        token.pos_jp      = feature[0]    # 品詞\n",
    "        token.pos_detail1 = feature[1]    # 品詞細分類1\n",
    "        token.pos_detail2 = feature[2]    # 品詞細分類2\n",
    "        token.pos_detail3 = feature[3]    # 品詞細分類3\n",
    "        token.infl_type   = feature[4]    # 活用型\n",
    "        token.infl_form   = feature[5]    # 活用形\n",
    "        token.base_form   = feature[6] if feature[6]!='*' else node.surface # 原型 ex)MacMini's base_form=='*'\n",
    "        token.pos         = POS_DIC.get( feature[0], 'X' )     # 品詞\n",
    "        token.reading     = feature[7] if len(feature) > 7 else ''  # 読み\n",
    "        token.phonetic    = feature[8] if len(feature) > 8 else ''  # 発音\n",
    "        # for BOS/EOS\n",
    "        if token.pos != 'EOS':#posがEOSでない場合\n",
    "            tokens.append(token)#tokensにtokenをappend\n",
    "            idx += 1#idxを+1(分かち書きの文節？の数)\n",
    "        node = node.next#次の\n",
    "    return tokens\n",
    "\n",
    "def normalize(src_text):\n",
    "    # Zenkaku to Hankaku ( handling japaneze character )\n",
    "    normalized = jctconv.h2z(src_text, digit=False, ascii=False) #半角を全角に\n",
    "    normalized = jctconv.z2h(normalized, kana=False)#全角を半角に\n",
    "    return normalized.lower()#小文字に変換して返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946f814acabc47beb69a02658fbdb64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7367), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#全ての文章をget_sentencesで分ける\n",
    "#text_s=[]\n",
    "text_use=[]\n",
    "for tex in tqdm(texts):\n",
    "    #tex = ''.join(tex.split()) \n",
    "    text_use.append(get_sentences(tex))\n",
    "    #text_s.append(text)\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-127e8be393e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext_backup3\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtexts_title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_use\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtext_use\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtexts_original\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#必要なし、5単語より短い文を削除しようとしてやめた\n",
    "text_backup1=text_use\n",
    "text_backup2= texts_original\n",
    "text_backup3= texts_title\n",
    "for y in range(len(text_use)):\n",
    "    if len(text_use[y])<=5:\n",
    "        text_use.pop(y)\n",
    "        texts_original.pop(y)\n",
    "        texts_title.pop(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7213\n",
      "7213\n"
     ]
    }
   ],
   "source": [
    "print(len(text_use))\n",
    "print(len(texts_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ma_parceでtopicの算出に用いるための名詞の抽出を行う結果はwordsとしてリスト化\n",
    "\n",
    "\n",
    "mecab = MeCab.Tagger(\"-Ochasen -d /usr/local/mecab/lib/mecab/dic/mecab-ipadic-neologd/\")\n",
    "mecab.parse('')\n",
    "# MeCabを使って形態素解析をします。\n",
    "def ma_parse(sentence, filter=\"名詞\",filter2=\"名詞,非自立\",filter3=\"名詞,固有\",filter4=\"名詞,代名詞\",filter5=\"名詞,接尾\",filter6=\"名詞,数\"):\n",
    "    node = mecab.parseToNode(sentence)\n",
    "    while node:\n",
    "        if node.feature.startswith(filter) and not (node.feature.startswith(filter2))  and not (node.feature.startswith(filter4)) and not (node.feature.startswith(filter5)) and not (node.feature.startswith(filter6)) and node.surface!=\"自分\":\n",
    "            yield node.surface\n",
    "        node = node.next\n",
    "\n",
    "words=[]\n",
    "words_use_corpus=[]\n",
    "for i in range(len(text_use)):\n",
    "    word=[]\n",
    "    for s in range(len(text_use[i])):\n",
    "        words_use_corpus.append([word for word in ma_parse(text_use[i][s])]) \n",
    "        word.append([word for word in ma_parse(text_use[i][s])])\n",
    "    words.append(word)\n",
    "#print(words_use_corpus)\n",
    "#print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ldaでそれぞれの文章のtopicの元になるldaを求める\n",
    "topic_number=8\n",
    "from gensim import models, corpora\n",
    "dictionary = corpora.dictionary.Dictionary(words_use_corpus)\n",
    "corpus = [dictionary.doc2bow(count) for count in words_use_corpus]\n",
    "#print(corpus)\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, num_topics=topic_number, id2word=dictionary)\n",
    "\n",
    "# Topics\n",
    "#for topic in lda.show_topics(-1):\n",
    "    #print('topic')\n",
    "    #print(topic)\n",
    "\n",
    "# Topic of each document\n",
    "#for topics_per_document in lda[corpus]:\n",
    "    #print('topic of ecah document')\n",
    "    #print(topics_per_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#話し言葉変換のための辞書作成(endが終助詞、hukuが副詞,keiyouが形容詞,setuが接続詞,sonotaがその他（いらないかもしれない）,zyoshiが助詞,meishiが名詞)\n",
    "pd.set_option('display.max_columns', 10000)#ディスプレイにおいて表示されるpandasの数\n",
    "pd.set_option('display.max_rows', 10000)#上記と同じ\n",
    "#df_none = pd.read_csv('pn.csv', sep=\"\\t\",names=('A', 'B','C'))#pandasで読み込み\n",
    "pn_end=pd.read_table('./for_conversion/sent_talk_end.txt',names=('A','B'),sep=\",\")#同上\n",
    "pn_huku=pd.read_table('./for_conversion/sent_talk_huku.txt',names=('A','B'),sep=\",\")#同上\n",
    "pn_keiyou=pd.read_table('./for_conversion/sent_talk_keiyou.txt',names=('A','B'),sep=\",\")#同上\n",
    "pn_setu=pd.read_table('./for_conversion/sent_talk_setu.txt',names=('A','B'),sep=\",\")#同上\n",
    "pn_sonota=pd.read_table('./for_conversion/sent_talk_sonota1.txt',names=('A','B'),sep=\",\")#同上\n",
    "pn_zyoshi=pd.read_table('./for_conversion/sent_talk_zyosi.txt',names=('A','B'),sep=\",\")#同上\n",
    "pn_meishi=pd.read_table('./for_conversion/sent_talk_meishi.txt',names=('A','B'),sep=\",\")#同上\n",
    "        #s=df_none.spit(\"¥t\",expand=True)\n",
    "        #pn=pn.T\n",
    "#df_none_dict= dict([(i,a) for i, a in zip(df_none.A,df_none.B)])\n",
    "#話し言葉変換の辞書作成\n",
    "pn_answer_end=dict([(a,i) for i, a in zip(pn_end.A,pn_end.B)])\n",
    "pn_answer_huku=dict([(a,i) for i, a in zip(pn_huku.A,pn_huku.B)])\n",
    "pn_answer_keiyou=dict([(a,i) for i, a in zip(pn_keiyou.A,pn_keiyou.B)])\n",
    "pn_answer_setu=dict([(a,i) for i, a in zip(pn_setu.A,pn_setu.B)])\n",
    "pn_answer_sonota=dict([(a,i) for i, a in zip(pn_sonota.A,pn_sonota.B)])\n",
    "pn_answer_zyoshi=dict([(a,i) for i, a in zip(pn_zyoshi.A,pn_zyoshi.B)])\n",
    "pn_answer_meishi=dict([(a,i) for i, a in zip(pn_meishi.A,pn_meishi.B)])\n",
    "#pn_answer_end.update(pn_answer_huku)#pn_answer連結\n",
    "#pn_answer_end.update(pn_answer_keiyou)\n",
    "#pn_answer_end.update(pn_answer_setu)\n",
    "#pn_answer_end.update(pn_answer_sonota)\n",
    "#pn_answer_end[\"など\"]=\"とか\"\n",
    "#print(pn_answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#絵文字辞書作成\n",
    "pd.set_option('display.max_columns', 10000)#ディスプレイにおいて表示されるpandasの数\n",
    "pd.set_option('display.max_rows', 10000)#上記と同じ\n",
    "#p=list(range(40))\n",
    "#print(p)\n",
    "with codecs.open(\"./for_conversion/kazyo_emoji.txt\", \"r\", \"Shift-JIS\", \"ignore\") as file:\n",
    "    df=pd.read_table(file,names=('A','B','C'))\n",
    "    #df = pd.read_table(file, delimiter=\",\")\n",
    "df_none1 = pd.read_csv('./for_conversion/kanzyo_emoji1.csv')#pandasで読み込み\n",
    "df_none2=pd.read_csv('./for_conversion/kanzyo_emoji2.csv')#pandasで読み込み\n",
    "df_none3=pd.read_csv('./for_conversion/kanzyo_emoji3.csv')\n",
    "kanzyo_emoji_dict1= dict([(i,a) for i, a in zip(df_none1.Word,df_none1.Emotion)])\n",
    "kanzyo_emoji_dict2=dict([(i,a) for i, a in zip(df_none2.Word,df_none2.Emotion)])\n",
    "kanzyo_emoji_dict3=dict([(i,a) for i, a in zip(df_none3.Word,df_none3.Emotion)])\n",
    "emoji_dict=dict([(i,a) for i, a in zip(df.B,df.C)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#絵文字辞書を絵文字と感情のラベルと、ラベルとその言葉の二つの辞書の作成\n",
    "#print(emoji_dict)\n",
    "#print(df)\n",
    "#print(df_none1)\n",
    "#print(df_none2)\n",
    "#print(kanzyo_emoji_dict1)\n",
    "#print(kanzyo_emoji_dict2)\n",
    "#print(kanzyo_emoji_dict3)\n",
    "dict_emoji0=list(kanzyo_emoji_dict1.keys())\n",
    "dict_emoji1=list(kanzyo_emoji_dict1.values())\n",
    "dict_emoji2=list(kanzyo_emoji_dict2.values())\n",
    "dict_emoji3=list(kanzyo_emoji_dict3.values())\n",
    "for i in range(len(dict_emoji1)):\n",
    "    dict_emoji1[i]=dict_emoji1[i]+dict_emoji2[i]+dict_emoji3[i]\n",
    "#print(dict_emoji0)\n",
    "#print(dict_emoji1)\n",
    "dict_emoji0.pop()\n",
    "dict_emoji1.pop()\n",
    "kanzyo_emoji_dict=dict(zip(dict_emoji0,dict_emoji1))\n",
    "#print(kanzyo_emoji_dict)\n",
    "#感情語ラベルのカウントをし一番多いものにする。\n",
    "for key,value in kanzyo_emoji_dict.items():\n",
    "    mycounter=Counter(value)\n",
    "    most1=mycounter.most_common(1)\n",
    "    kanzyo_emoji_dict[key]=most1[0][0]\n",
    "\n",
    "#print(kanzyo_emoji_dict)\n",
    "#for i,w1 in enumerate(list(pn_answer_end.keys())):#辞書のキーで回す\n",
    "        #print(w1)\n",
    "        #if w1 in word_sent:#w1がword_sentのなかにあれば\n",
    "            #word_sent=word_sent.replace(w1,dict1[i])#w1をdict[i]で置き換え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lex_Rankコード\n",
    "mecab = MeCab.Tagger (\"-Owakati -d /usr/local/mecab/lib/mecab/dic/mecab-ipadic-neologd/\")\n",
    "model_path = './word2vec/word2vec.gensim.model'\n",
    "word2vec_model = gensim.models.Word2Vec.load(model_path)\n",
    "def lex_rank(sentences, n, threshold,rate):\n",
    "    \"\"\"\n",
    "    LexRankで文章を要約する．\n",
    "    @param  sentences: list\n",
    "        文章([[w1,w2,w3],[w1,w3,w4,w5],..]のような文リスト)\n",
    "    @param  n: int\n",
    "        文章に含まれる文の数\n",
    "    @param  t: float\n",
    "        コサイン類似度の閾値(default 0.1)\n",
    "    @return : list\n",
    "        LexRank\n",
    "    \"\"\"\n",
    "    cosine_matrix = numpy.zeros((n, n))#n×nのゼロ行列二次元\n",
    "    degrees = numpy.zeros((n,))#n×零行列\n",
    "    l = []\n",
    "\n",
    "     # 1. 隣接行列の作成\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            cosine_matrix[i][j] = sentence_similarity(sentences[i], sentences[j])#隣接文章のコサイン類似度の行列\n",
    "            if cosine_matrix[i][j] > threshold:#闘値より大きいならばコサイン類似度１degreeに＋１\n",
    "                cosine_matrix[i][j] = 1\n",
    "                degrees[i] += 1\n",
    "            else:\n",
    "                cosine_matrix[i][j] = 0#闘値以下なら０\n",
    "    #print(cosine_matrix)\n",
    "    #print(degrees)\n",
    "    # 2.LexRank計算\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if degrees[i] != 0:\n",
    "                cosine_matrix[i][j] = cosine_matrix[i][j] / degrees[i]#角度でコサイン類似度をわる（角度が大きいと類似度が小さい）\n",
    "    #print(cosine_matrix)\n",
    "    ratings=[]\n",
    "    ratings=power_method(cosine_matrix,n,rate)\n",
    "    #print(ratings)\n",
    "    #print(sentences)\n",
    "    #for i ,t in enumerate(ratings):\n",
    "        #if(senti_score[i]>=0):\n",
    "            #ratings[i]=math.sqrt(t*senti_score[i])#レイト付\n",
    "        #elif(senti_score[i]<0):\n",
    "            #ratings[i]=math.sqrt(t*senti_score[i]*-1)*-1\n",
    "    \n",
    "    #print(ratings)\n",
    "    #print(ratings)\n",
    "\n",
    "    return zip(sentences, ratings)\n",
    "\n",
    "def avg_feature_vector(sentence, model, num_features):\n",
    "    words = mecab.parse(sentence).replace(' \\n', '').split() # mecabの分かち書きでは最後に改行(\\n)が出力されてしまうため、除去\n",
    "    feature_vec = numpy.zeros((num_features,), dtype=\"float32\") # 特徴ベクトルの入れ物を初期化\n",
    "    for word in words:\n",
    "        try:\n",
    "            feature_vec = numpy.add(feature_vec, model[word])\n",
    "        except:\n",
    "            continue\n",
    "    #print(feature_vec)\n",
    "    if len(words) > 0:\n",
    "        feature_vec = numpy.divide(feature_vec, len(words))\n",
    "    return feature_vec\n",
    "\n",
    "sent_vec=[]\n",
    "def sentence_similarity(sentence_1, sentence_2):\n",
    "    # 今回使うWord2Vecのモデルは50次元の特徴ベクトルで生成されているので、num_featuresも50に指定\n",
    "    num_features=50\n",
    "    \n",
    "    sentence_1_avg_vector = avg_feature_vector(sentence_1, word2vec_model, num_features)\n",
    "    sentence_2_avg_vector = avg_feature_vector(sentence_2, word2vec_model, num_features)\n",
    "    # １からベクトル間の距離を引いてあげることで、コサイン類似度を計算\n",
    "    sent_vec.append(1 - spatial.distance.cosine(sentence_1_avg_vector, sentence_2_avg_vector))\n",
    "    return 1 - spatial.distance.cosine(sentence_1_avg_vector, sentence_2_avg_vector)\n",
    "\n",
    "def power_method(cosine_matrix, n, e):\n",
    "    \"\"\"\n",
    "    べき乗法を行なう\n",
    "    @param  scosine_matrix: list\n",
    "        確率行列\n",
    "    @param  n: int\n",
    "        文章中の文の数\n",
    "    @param  e: float\n",
    "        許容誤差ε\n",
    "    @return: list\n",
    "        LexRank\n",
    "    \"\"\"\n",
    "    transposed_matrix = cosine_matrix.T#転置\n",
    "    #print(transposed_matrix)\n",
    "    sentences_count = n#文章の数\n",
    "\n",
    "    p_vector = numpy.array([1.0 / sentences_count] * sentences_count)#nの文章分の行列のアーレイ\n",
    "    #print(p_vector)\n",
    "\n",
    "    lambda_val = 1.0 #(ラムダの大きさ)\n",
    "\n",
    "    while lambda_val > e:#ラムダの大きさが許容誤差より大きい場合\n",
    "        next_p = numpy.dot(transposed_matrix, p_vector)#コサイン類似度の転置行列とPベクトルの内積\n",
    "        #print(next_p)\n",
    "        lambda_val = numpy.linalg.norm(numpy.subtract(next_p, p_vector))#next_p-p_vectorのノルム計算\n",
    "        \n",
    "        p_vector = next_p#次のPへ\n",
    "    \n",
    "\n",
    "    return p_vector\n",
    "\n",
    "def mmr(lambda_, kotae,len_kotae):\n",
    "    mmr_rating=[]\n",
    "    Di=[]\n",
    "    for i in range(len_kotae):#答えの長さで回す\n",
    "        if Di != []:#選択された文章がある時\n",
    "            use_mmr=[]\n",
    "            for ds in range(len(DS)):#選択されていない文\n",
    "                for di in range(len(Di)):#選択されている文\n",
    "                    #print(DS[ds][0])\n",
    "                    use_mmr.append(lambda_*DS[ds][1] - (1 - lambda_) * sentence_similarity(DS[ds][0],Di[di][0]))#mmr計算\n",
    "                    #use_mmr=sorted(use_mmr)#mmrのソート\n",
    "                    #mmr_rating.append(use_mmr[0])#mmr_ratingの更新\n",
    "                    #use_mmr=[]\n",
    "            use_kotae=sorted(list(zip(use_mmr,DS)))#mmrでソート(もっとも値が大きいものを選択するため)\n",
    "            mmr_rating,use_kotae=list(zip(*use_kotae))#zipを解除\n",
    "            use_kotae=list(use_kotae)#list化\n",
    "            mmr_rating=list(mmr_rating)#list化\n",
    "            DS=[]\n",
    "            Di.append(use_kotae[0])#ソートの最大をDi(選択された文章に入れる)\n",
    "            use_kotae.pop(0)#選択されたものをポップ\n",
    "            for i in use_kotae:\n",
    "                DS.append(i)#DS選択されていない文章の更新\n",
    "        else:\n",
    "            DS=[]\n",
    "            Di.append(kotae[0])#選択される文にLexRankの結果を入れる\n",
    "            for i in kotae:\n",
    "                DS.append(i)#DSに入れる\n",
    "            #print(DS)\n",
    "            DS.pop(0)#DSのDiに加えたものをpopする。\n",
    "            #print(DS)\n",
    "        \n",
    "    return Di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semanticVここから\n",
    "\n",
    "# MeCabで形態素に分割\n",
    "def split_words(sentence):\n",
    "    result = []\n",
    "    available_pos = [\"名詞\", \"動詞-自立\", \"形容詞\"]\n",
    "    not_available_pos = [\"名詞-数\"]\n",
    "    tokenizer = MeCab.Tagger('-Ochasen -d /usr/local/mecab/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "    chasen_result = tokenizer.parse(sentence)#解析結果を文字列で取得\n",
    "    for line in chasen_result.split(\"\\n\"):#上のものを改行で分割\n",
    "        elems = line.split(\"\\t\")#空白で区切る\n",
    "        if len(elems) < 4:#長さが４以下ならば続行\n",
    "            continue\n",
    "        word = elems[0]#言葉\n",
    "        pos = elems[3]#上記の分類\n",
    "        if True in [pos.startswith(w) for w in not_available_pos]:#posがavaila---内の文字列で始まるかどうか\n",
    "            continue\n",
    "        if True in [pos.startswith(w) for w in available_pos]:#posがnot---内の文字列で始まるかどうか\n",
    "            result.append(word)#結果に名詞、動詞-自立、形容詞を格納\n",
    "    return result\n",
    "\n",
    "# 文章をベクトルに変換（単語の平均ベクトルを用いる）\n",
    "def wordvec2docmentvec(sentence,vec_model,num_features):\n",
    "    doc_vec = np.zeros(num_features, dtype=\"float32\")\n",
    "    for word in sentence:\n",
    "        try: #例外処理\n",
    "            temp = vec_model[word]#modelのなかの単語のベクトルを抽出\n",
    "        except:\n",
    "            continue\n",
    "        doc_vec += temp#文書のベクトルの合計を取る\n",
    "    if len(sentence)>0:#センテンスの長さが０以上だったら\n",
    "        doc_vec =  doc_vec / len(sentence)#文書の平均ベクトルを取る\n",
    "        \n",
    "    return doc_vec\n",
    "\n",
    "# Proj を計算\n",
    "def projection(u, b):\n",
    "    return np.dot(u,b) * b #uとbの内積×b\n",
    "\n",
    "# 基底ベクトル\n",
    "def basis_vector(v):\n",
    "    \n",
    "    return v / np.linalg.norm(v)#vをVのノルムで割る(基底ベクトル)\n",
    "\n",
    "#　Distance(u_i, B)\n",
    "def span_distance(v, span_space):\n",
    "    features=50\n",
    "    proj = np.zeros(features, dtype=\"float32\")#featureの数のゼロ行列\n",
    "    for span_vec in span_space:\n",
    "        proj += projection(v, span_vec)\n",
    "    return np.linalg.norm(v - proj)#ベクトル間の距離\n",
    "\n",
    "# index of sentence that is farthest from the subspace of Span(B).\n",
    "def compute_farthest_spanspace(sentences_vector, span_subspace, skip_keys):\n",
    "    distance = 0\n",
    "    farthest_key = 0\n",
    "    for i, vec in enumerate(sentences_vector): #インデックス付きのfor\n",
    "        if i in skip_keys:#iにskip_keysが入っていたら続ける\n",
    "            continue\n",
    "        dist = span_distance(vec, span_subspace) #ベクトル間の長さ\n",
    "        if dist >= distance:#もしも長さがdistanceより長かったら\n",
    "            distance = dist\n",
    "            farthest_key = i#もっとも大きいキー\n",
    "    return farthest_key\n",
    "\n",
    "def semanticVolume(sentence):\n",
    "    original_data = []\n",
    "    corpus = []\n",
    "    corpus_vec = []\n",
    "    model_path = './word2vec/word2vec.gensim.model'\n",
    "    model = Word2Vec.load(model_path)#日本語の学習済みモデル\n",
    "    features = 50\n",
    "\n",
    "    for i in sentence:\n",
    "        #print(i)\n",
    "        h = split_words(i)\n",
    "        if len(h) == 0:#hの長さが０ならばcontinue\n",
    "            continue\n",
    "        original_data.append(i)#original_dataにsentenceを入れる\n",
    "        corpus.append(\" \".join(h))#\" h1 h2 h3\"がかえる\n",
    "        corpus_vec.append(wordvec2docmentvec(h, model, features))#corpusvecにベクトルを入れる\n",
    "        #print(original_data1)\n",
    "        \n",
    "    L=3#要約文の数\n",
    "    yoyaku_index = [] # 要約対象の文章ID\n",
    "    # 重心を計算\n",
    "    centroid = np.zeros(features, dtype=\"float32\")\n",
    "    for vec in corpus_vec:\n",
    "        centroid += vec\n",
    "    centroid /= len(corpus_vec)\n",
    "    # index of sentence that is farthest from the cluster centroid\n",
    "    distance = 0\n",
    "    first_yoyaku_index = 0\n",
    "    for i, vec in enumerate(corpus_vec): \n",
    "        dist = np.linalg.norm(centroid - vec)\n",
    "        if dist >= distance:#距離が大きい場合\n",
    "            distance = dist\n",
    "            first_yoyaku_index = i#離れている文\n",
    "    \n",
    "    yoyaku_index.append(first_yoyaku_index)#文を作っていく\n",
    "    # index of sentence that is farthest from s_p\n",
    "    distance = 0\n",
    "    second_yoyaku_index = 0\n",
    "    \n",
    "    for i, vec in enumerate(corpus_vec):\n",
    "        if i in yoyaku_index:\n",
    "            continue\n",
    "        dist = np.linalg.norm(corpus_vec[first_yoyaku_index] - vec) #一番めのベクトルからベクトルを引く\n",
    "        if dist >= distance:#距離が大きければ\n",
    "            distance = dist\n",
    "            second_yoyaku_index = i#離れている\n",
    "    \n",
    "    yoyaku_index.append(second_yoyaku_index)\n",
    "    # total length\n",
    "    total_length = len(original_data[first_yoyaku_index]) + len(original_data[second_yoyaku_index])#文の全体の長さ\n",
    "    # first basis vector\n",
    "    first_basis_vector = basis_vector(corpus_vec[second_yoyaku_index])#最初の基底ベクトル\n",
    "    span_subspace = [first_basis_vector]#span_subspaceにbasisvectorを\n",
    "    \n",
    "    while(True):\n",
    "        farthest_index = compute_farthest_spanspace(corpus_vec, span_subspace, yoyaku_index)\n",
    "        if len(yoyaku_index) < L:#基底ベクトルの文とtotal_lengthの合計の文が200文字より小さければ\n",
    "            span_subspace.append(corpus_vec[farthest_index])#span_subspaceにfarthest_indexのcorpus_vecをappendする\n",
    "            total_length += len(original_data[farthest_index]) #全体の長さに最も遠いインデックスのオリジナルデータの長さをたす\n",
    "            yoyaku_index.append(farthest_index)#要約の文にアテンド\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    kotae=[]\n",
    "    indexs=[]\n",
    "    for i in yoyaku_index:\n",
    "        kotae.append(original_data[i])\n",
    "        indexs.append(i)\n",
    "    return zip(kotae,indexs)\n",
    "\n",
    "#SemanticVここまで------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic_Rank\n",
    "\n",
    "def js(topic1,topic2):\n",
    "    m = (topic1+topic2)/2\n",
    "    kl = entropy(topic1, topic2)\n",
    "    kl_pm = entropy(topic1, m)\n",
    "    kl_qm = entropy(topic2, m)\n",
    "    js = (kl_pm + kl_qm)/2\n",
    "    #print(js)\n",
    "    return js\n",
    "\n",
    "def power_method(topic_matrix, n, e):\n",
    "    \"\"\"\n",
    "    べき乗法を行なう\n",
    "    @param  scosine_matrix: list\n",
    "        確率行列\n",
    "    @param  n: int\n",
    "        文章中の文の数\n",
    "    @param  e: float\n",
    "        許容誤差ε\n",
    "    @return: list\n",
    "        LexRank\n",
    "    \"\"\"\n",
    "    transposed_matrix = topic_matrix.T#転置\n",
    "    #print(transposed_matrix)\n",
    "    sentences_count = n#文章の数\n",
    "\n",
    "    p_vector = np.array([1.0 / sentences_count] * sentences_count)#nの文章分の行列のアーレイ\n",
    "    #print(p_vector)\n",
    "\n",
    "    lambda_val = 1.0 #(ラムダの大きさ)\n",
    "\n",
    "    while lambda_val > e:#ラムダの大きさが許容誤差より大きい場合\n",
    "        next_p = np.dot(transposed_matrix, p_vector)#コサイン類似度の転置行列とPベクトルの内積\n",
    "        #print(next_p)\n",
    "        lambda_val = np.linalg.norm(np.subtract(next_p, p_vector))#next_p-p_vectorのノルム計算\n",
    "        #print(lambda_val)\n",
    "        p_vector = next_p#次のPへ\n",
    "        #print(p_vector)\n",
    "    \n",
    "\n",
    "\n",
    "    return p_vector\n",
    "\n",
    "def topic_rank(num1, n, t,Rate,sentence):\n",
    "    \"\"\"\n",
    "    LexRankで文章を要約する．\n",
    "    @param  sentences: list\n",
    "        文章([[w1,w2,w3],[w1,w3,w4,w5],..]のような文リスト)\n",
    "    @param  n: int\n",
    "        文章に含まれる文の数\n",
    "    @param  t: float\n",
    "        コサイン類似度の閾値(default 0.1)\n",
    "    @return : list\n",
    "        LexRank\n",
    "    \"\"\"\n",
    "    test_topicRank=words[num1]\n",
    "    topic=np.zeros((len(test_topicRank),8))\n",
    "    #print(lda[corpus][1])\n",
    "    for ei in range(len(test_topicRank)):\n",
    "        for ii in range(len(lda[corpus][ei])):\n",
    "            for si in range(len(lda[corpus][ei][ii])): \n",
    "                topic[ei][lda[corpus][ei][ii][0]]=lda[corpus][ei][ii][1]\n",
    "    \n",
    "    topic_matrix = np.zeros((n, n))#n×nのゼロ行列二次元\n",
    "    degrees = np.zeros((n,))#n×零行列\n",
    "    l = []\n",
    "\n",
    "     # 1. 隣接行列の作成\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            topic_matrix[i][j] = js(topic[i],topic[j])#隣接文章のtopic類似度の行列\n",
    "            if topic_matrix[i][j] > t:#闘値より大きいならばコサイン類似度１degreeに＋１\n",
    "                topic_matrix[i][j] = 1\n",
    "                degrees[i] += 1\n",
    "            else:\n",
    "                topic_matrix[i][j] = 0#闘値以下なら０\n",
    "    #print(topic_matrix)\n",
    "    #print(degrees)\n",
    "    # 2.LexRank計算\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if degrees[i] != 0:\n",
    "                topic_matrix[i][j] = topic_matrix[i][j] / degrees[i]#角度でコサイン類似度をわる（角度が大きいと類似度が小さい）\n",
    "    #print(topic_matrix)\n",
    "\n",
    "    ratings = power_method(topic_matrix, n,Rate)#レイト付\n",
    "   \n",
    "    #print(ratings)\n",
    "    #print(indexes)\n",
    "    kotae=zip(sentence,ratings)\n",
    "    #print(list(kotae))\n",
    "    #indexes =mmr_sort(lambda_,kotae,limit)\n",
    "    return kotae\n",
    "\n",
    "def mmr(lambda_, kotae,len_kotae):\n",
    "    mmr_rating=[]\n",
    "    Di=[]\n",
    "    for i in range(len_kotae):#答えの長さで回す\n",
    "        if Di != []:#選択された文章がある時\n",
    "            use_mmr=[]\n",
    "            for ds in range(len(DS)):#選択されていない文\n",
    "                for di in range(len(Di)):#選択されている文\n",
    "                    #print(DS[ds][0])\n",
    "                    use_mmr.append(lambda_*DS[ds][1] - (1 - lambda_) * sentence_similarity(DS[ds][0],Di[di][0]))#mmr計算\n",
    "                    #use_mmr=sorted(use_mmr)#mmrのソート\n",
    "                    #mmr_rating.append(use_mmr[0])#mmr_ratingの更新\n",
    "                    #use_mmr=[]\n",
    "            use_kotae=sorted(list(zip(use_mmr,DS)))#mmrでソート(もっとも値が大きいものを選択するため)\n",
    "            mmr_rating,use_kotae=list(zip(*use_kotae))#zipを解除\n",
    "            use_kotae=list(use_kotae)#list化\n",
    "            mmr_rating=list(mmr_rating)#list化\n",
    "            DS=[]\n",
    "            Di.append(use_kotae[0])#ソートの最大をDi(選択された文章に入れる)\n",
    "            use_kotae.pop(0)#選択されたものをポップ\n",
    "            for i in use_kotae:\n",
    "                DS.append(i)#DS選択されていない文章の更新\n",
    "        else:\n",
    "            DS=[]\n",
    "            Di.append(kotae[0])#選択される文にLexRankの結果を入れる\n",
    "            for i in kotae:\n",
    "                DS.append(i)#DSに入れる\n",
    "            #print(DS)\n",
    "            DS.pop(0)#DSのDiに加えたものをpopする。\n",
    "            #print(DS)\n",
    "        \n",
    "    return Di\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#話し言葉、絵文字、敬語への変換\n",
    "\n",
    "from pykakasi import kakasi\n",
    "def conversion(text,switch):\n",
    "    word_sent=\"\"\n",
    "    unuse_words=[]\n",
    "    flag=0\n",
    "    #text=normalize(text)\n",
    "    #print(text)\n",
    "    tagger = MeCab.Tagger ('-Ochasen /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "    result = tagger.parse(\"\")\n",
    "    node = tagger.parseToNode(text)\n",
    "    tokens = list()\n",
    "    idx = 0\n",
    "    POS_DIC = {\n",
    "            'BOS/EOS': 'EOS', # end of sentense\n",
    "            '形容詞' : 'ADJ',\n",
    "            '連体詞' : 'JADJ', # Japanese-specific POS like a adjective\n",
    "            '副詞'   : 'ADV',\n",
    "            '名詞'   : 'NOUN',\n",
    "            '動詞'   : 'VERB',\n",
    "            '助動詞' : 'AUX',\n",
    "            '助詞'   : 'PART',\n",
    "            '感動詞' : 'INTJ',\n",
    "            '接続詞' : 'CONJ',\n",
    "            '記号'   : 'SYM', # symbol\n",
    "            '*'      : 'X',\n",
    "            'その他' : 'X',\n",
    "            'フィラー': 'X',\n",
    "            '接頭詞' : 'X',\n",
    "        }\n",
    "    LOMA_TO_KANA={\n",
    "        'ai':'い',\n",
    "        'ii':'い',\n",
    "        'ui':'い',\n",
    "        'ei':'い',\n",
    "        'oi':'い',\n",
    "        'ki':'き',\n",
    "        'si' : 'し',\n",
    "        'ti' : 'ち',\n",
    "        'ni' : 'に',\n",
    "        'hi' : 'ひ',\n",
    "        'mi' : 'み',\n",
    "        'ri' : 'り',\n",
    "        'gi' : 'ぎ',\n",
    "        'zi' : 'じ',\n",
    "        'ji' : 'じ',\n",
    "        'di' : 'ぢ',\n",
    "        'bi' : 'び',\n",
    "        'pi' : 'ぴ',\n",
    "        'ae':'え',\n",
    "        'ie':'え',\n",
    "        'ue':'え',\n",
    "        'ee':'え',\n",
    "        'oe':'え',\n",
    "        'ke':'け',\n",
    "        'se':'せ',\n",
    "        'te':'て',\n",
    "        'ne':'ね',\n",
    "        'he':'へ',\n",
    "        'me':'め',\n",
    "        're':'れ',\n",
    "        'ge' : 'げ',\n",
    "        'ze' : 'ぜ',\n",
    "        'de' : 'で',\n",
    "        'be' : 'べ',\n",
    "        'pe' : 'ぺ',\n",
    "        }\n",
    "    i=0\n",
    "    answer_sent_1=[]\n",
    "    while node :\n",
    "        \n",
    "        feature = node.feature.split(',')\n",
    "        token = namedtuple('Token', 'idx, surface, pos, pos_detail1, pos_detail2, pos_detail3,\\\n",
    "        infl_type, infl_form, base_form, reading, phonetic')\n",
    "        token.idx         = idx\n",
    "        token.surface     = node.surface  # 表層形\n",
    "        #print(node.surface)\n",
    "        token.pos_jp      = feature[0]    # 品詞\n",
    "        token.pos_detail1 = feature[1]    # 品詞細分類1\n",
    "        token.pos_detail2 = feature[2]    # 品詞細分類2\n",
    "        token.pos_detail3 = feature[3]    # 品詞細分類3\n",
    "        token.infl_type   = feature[4]    # 活用型\n",
    "        token.infl_form   = feature[5]    # 活用形\n",
    "        token.base_form   = feature[6] if feature[6]!='*' else node.surface # 原型 ex)MacMini's base_form=='*'\n",
    "        token.pos         = POS_DIC.get( feature[0], 'X' )     # 品詞\n",
    "        token.reading     = feature[7] if len(feature) > 7 else ''  # 読み\n",
    "        token.phonetic    = feature[8] if len(feature) > 8 else ''  # 発音\n",
    "        #print(token.pos)\n",
    "        #print(token.surface)\n",
    "        #print(feature)\n",
    "        # for BOS/EOS\n",
    "        if token.pos != 'EOS':\n",
    "            tokens.append(token)\n",
    "            idx += 1\n",
    "       \n",
    "        #print(\"-----------------------------\"+token.surface)\n",
    "        \n",
    "        \n",
    "       \n",
    "        #print(word_sent)\n",
    "        if switch ==\"talk\" or switch==\"emoji\":\n",
    "            if (token.pos==\"CONJ\" or (token.pos==\"PART\" and token.pos_detail1==\"接続助詞\")) and token.surface in pn_answer_setu:\n",
    "                token.surface=pn_answer_setu[token.surface]\n",
    "                #print(token.surface)\n",
    "            elif token.pos==\"ADV\" and token.surface in pn_answer_huku:\n",
    "                token.surface=pn_answer_huku[token.surface]\n",
    "                #print(token.surface)\n",
    "            elif token.pos==\"ADJ\" and token.surface in pn_answer_keiyou:\n",
    "                token.surface=pn_answer_keiyou[token.surface]\n",
    "                #print(token.surface)\n",
    "            elif token.pos==\"PART\" and token.surface in pn_answer_zyoshi:\n",
    "                token.surface=pn_answer_zyoshi[token.surface]\n",
    "            elif token.pos==\"NOUN\" and token.surface in pn_answer_meishi:\n",
    "                token.surface=pn_answer_meishi[token.surface]\n",
    "                #print(\"=====\")\n",
    "            dict1=list(pn_answer_end.values())\n",
    "            if len(answer_sent_1)>2:\n",
    "                if token.surface in ['。', '．', '！' ] and answer_sent_1[-1] not in [\"か\",\"なあ\",\"や\",\"ぞ\",\"よ\",\"さ\",\"ね\"] and tokens[-2].pos!=\"NOUN\":\n",
    "                    answer_sent_1.append(\"よ\")\n",
    "            answer_sent_1.append(token.surface)\n",
    "            word_sent=\"\".join(answer_sent_1)\n",
    "            for i,w1 in enumerate(list(pn_answer_end.keys())):#辞書のキーで回す\n",
    "                #print(w1)\n",
    "                if w1 in word_sent:#w1がword_sentのなかにあれば\n",
    "                    word_sent=word_sent.replace(w1,dict1[i])#w1をdict[i]で置き換え\n",
    "                    \n",
    "            #print(w1)\n",
    "        if switch ==\"emoji\":\n",
    "            for i,w1 in enumerate(list(kanzyo_emoji_dict.keys())):#辞書のキーで回す\n",
    "                if w1 in word_sent:#w1がword_sentのなかにあれば\n",
    "                    for t in list(kanzyo_emoji_dict.values())[i]:\n",
    "                        if t in emoji_dict and emoji_dict[t] not in unuse_words:\n",
    "                            answer_sent_1.append(emoji_dict[t])\n",
    "                            unuse_words.append(emoji_dict[t])\n",
    "           \n",
    "        if switch==\"nomal\":\n",
    "            answer_sent_1.append(token.surface)\n",
    "            word_sent=\"\".join(answer_sent_1)\n",
    "        node = node.next\n",
    "        #print(tokens)\n",
    "    #global kakasi\n",
    "    if switch==\"keigo\" :\n",
    "       \n",
    "\n",
    "        if len(tokens)>=3:\n",
    "            #print(tokens[-3].pos)\n",
    "            for index in [-2,-3]:\n",
    "                #print(tokens[-1].surface)\n",
    "                if tokens[-1].surface in ['。', '．', '！' ,'！'] and tokens[index].pos=='VERB' :\n",
    "                    global kakasi1\n",
    "                    kakasi1 = kakasi()  # Generate kakasi instance\n",
    "                    kakasi1.setMode(\"H\", \"a\")  # Hiragana to ascii\n",
    "                    kakasi1.setMode(\"K\", \"a\")  # Katakana to ascii\n",
    "                    kakasi1.setMode(\"J\", \"a\")  # Japanese(kanji) to ascii\n",
    "                    kakasi1.setMode(\"r\", \"Hepburn\")  # Use Hepburn romanization\n",
    "                    conv = kakasi1.getConverter()\n",
    "                    kana=conv.do(tokens[index].base_form)\n",
    "                    #print(index)\n",
    "                    #print(tokens[index].infl_type[0:1])\n",
    "                    if tokens[index].infl_type[0:2]==\"五段\" or tokens[index].infl_type[0:3]==\"上一段\":\n",
    "                        kana_use=LOMA_TO_KANA[kana[-2]+\"i\"]\n",
    "                        tokens[index].surface=tokens[index].surface[:-1]+kana_use\n",
    "                    \n",
    "                    elif tokens[index].infl_type[0:3]==\"下一段\":\n",
    "                        kana_use=LOMA_TO_KANA[kana[-2]+\"e\"]\n",
    "                        tokens[index].surface=tokens[index].surface[:-1]+kana_use\n",
    "                    \n",
    "                    elif tokens[index].infl_type[0:2]==\"カ変\" :\n",
    "                        tokens[index].surface=\"来\"\n",
    "                    elif tokens[index].infl_type[0:2]==\"サ変\" :\n",
    "                        tokens[index].surface=\"し\"\n",
    "                    elif tokens[index].infl_type[0:2]==\"一段\":\n",
    "                    \n",
    "                        if \"る\" in tokens[index].surface:\n",
    "                            tokens[index].surface=tokens[index].surface.strip(\"る\")\n",
    "                \n",
    "                    if index==-3 :\n",
    "                        if tokens[-2].surface=='た' :\n",
    "                            tokens[-2].surface='ました'\n",
    "                       \n",
    "                        elif tokens[-2].surface=='ない' :\n",
    "                            tokens[-2].surface='ません'\n",
    "                        else:\n",
    "                            tokens[-2].surface='ます'\n",
    "                        \n",
    "                    elif index==-2 :\n",
    "                        tokens[-1].surface='ます。'\n",
    "                        break\n",
    "                elif tokens[index].pos==\"ADJ\":\n",
    "                    tokens[-1].surface='です。'\n",
    "            #print(tokens[-1].surface)\n",
    "            if tokens[-2].pos==\"NOUN\":\n",
    "                    tokens[-1].surface=\"です。\"\n",
    "            \n",
    "            \n",
    "            if tokens[-2].surface==\"だ\":\n",
    "                tokens[-2].surface=\"です\"\n",
    "        \n",
    "        \n",
    "            if len(tokens)>=4 and tokens[-4].surface+tokens[-3].surface==\"だろう\" and tokens[-2].surface==\"か\" and tokens[-1].surface in [\"?\",\"？\"]:\n",
    "                tokens[-4].surface=\"でしょう\"\n",
    "                tokens.pop(-3)\n",
    "            elif tokens[-2].surface==\"か\" and tokens[-1].surface in [\"?\",\"？\"]:\n",
    "                tokens[-2].surface=\"でしょうか\"\n",
    "        \n",
    "       \n",
    "        \n",
    "                #if(tokens[-2].surface=='た'):\n",
    "                    #tokens[-1].surface='です。'\n",
    "                #elif(tokens[-2].surface!='た'):\n",
    "                    #tokens[index].surface=tokens[index].base_form\n",
    "                    #if(index==-3):\n",
    "                        #tokens[-2].surface='です'\n",
    "                    #elif(index==-2):\n",
    "                        #tokens[-1].surface='です。'\n",
    "                    \n",
    "                    \n",
    "        for i in tokens:\n",
    "            answer_sent_1.append(i.surface)\n",
    "            word_sent=\"\".join(answer_sent_1)\n",
    "        #print(word_sent)\n",
    "    return word_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54]\n",
      "[52, 40, 46, 49, 37, 42, 40, 48, 37, 52, 38, 53, 41, 51, 50, 39, 50, 37, 54, 40, 42, 41, 46, 43, 44, 43, 46, 49, 50, 42, 54, 51, 50, 38, 52, 51, 49, 54, 44, 51, 45, 47, 52, 44, 38, 42, 47, 53, 46, 45, 48, 49, 53, 37, 40, 37, 42, 45, 38, 38, 41, 43, 48, 53, 41, 54, 43, 50, 51, 37, 39, 48, 39, 49, 45, 38, 39, 43, 47, 42, 41, 52, 44, 44, 49, 39, 42, 38, 38, 51, 41, 49, 54, 48, 39, 44, 39, 52, 44, 39, 51, 41, 43, 50, 54, 44, 37, 52, 45, 53, 50, 41, 51, 54, 45, 52, 44, 42, 37, 47, 53, 45, 48, 47, 43, 47, 43, 50, 44, 42, 52, 37, 38, 40, 50, 45, 54, 46, 53, 53, 54, 40, 40, 47, 53, 45, 54, 43, 41, 51, 51, 41, 39, 47, 40, 40, 49, 50, 45, 46, 38, 43, 47, 40, 49, 48, 48, 52, 37, 48, 48, 53, 42, 49, 47, 39, 46, 46, 46, 46]\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "shuffle=list(range(180))\n",
    "for u in range(180):\n",
    "    if(u in range(0,10)):\n",
    "        shuffle[u]=37\n",
    "    elif(u in range(10,20)):\n",
    "        shuffle[u]=38\n",
    "    elif(u in range(20,30)):\n",
    "        shuffle[u]=39\n",
    "    elif(u in range(30,40)):\n",
    "        shuffle[u]=40\n",
    "    elif(u in range(40,50)):\n",
    "        shuffle[u]=41\n",
    "    elif(u in range(50,60)):\n",
    "        shuffle[u]=42\n",
    "    elif(u in range(60,70)):\n",
    "        shuffle[u]=43\n",
    "    elif(u in range(70,80)):\n",
    "        shuffle[u]=44\n",
    "    elif(u in range(80,90)):\n",
    "        shuffle[u]=45\n",
    "    elif(u in range(90,100)):\n",
    "        shuffle[u]=46\n",
    "    elif(u in range(100,110)):\n",
    "        shuffle[u]=47\n",
    "    elif(u in range(110,120)):\n",
    "        shuffle[u]=48\n",
    "    elif(u in range(120,130)):\n",
    "        shuffle[u]=49\n",
    "    elif(u in range(130,140)):\n",
    "        shuffle[u]=50\n",
    "    elif(u in range(140,150)):\n",
    "        shuffle[u]=51\n",
    "    elif(u in range(150,160)):\n",
    "        shuffle[u]=52\n",
    "    elif(u in range(160,170)):\n",
    "        shuffle[u]=53\n",
    "    elif(u in range(170,180)):\n",
    "        shuffle[u]=54\n",
    "\n",
    "print(shuffle)\n",
    "random.shuffle(shuffle)\n",
    "print(shuffle)\n",
    "print(len(shuffle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc958554da04cd69f7f04387ace993b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.seki/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '52/Lex_talk日本3Dメガネドレッサー賞で仰天発言「3Dメガネで安めぐみの体を見たい」.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8af7ef0b001c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mtext_answer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_answer\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myouyaku\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mtext_answer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_answer\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_original\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mway\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mchange\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[\\\\|/|:|?|.|\"|<|>|\\|]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts_title\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# w:上書きモード\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '52/Lex_talk日本3Dメガネドレッサー賞で仰天発言「3Dメガネで安めぐみの体を見たい」.txt'"
     ]
    }
   ],
   "source": [
    "#結果のファイル 化。\n",
    "ritu=0.85\n",
    "ikiti=0.7\n",
    "rate=0.95\n",
    "ikiti_topic=1.0\n",
    "num_sent=360\n",
    "count1=99\n",
    "i=0\n",
    "i_use=0\n",
    "i2=0\n",
    "www=0\n",
    "abstway=0\n",
    "for i2 in tqdm(range(180)):\n",
    "    #print(i)\n",
    "    i=i2+num_sent\n",
    "    i_use=i2\n",
    "    answer=[]\n",
    "    #print(\"i2の結果:\"+str(i2))\n",
    "    #print(\"i_useの結果:\"+str(i_use))\n",
    "    #print(text_use[691])\n",
    "    if(i_use in range(0,20)):\n",
    "        answer=sorted(list(lex_rank(text_use[i],len(text_use[i]),ikiti,ritu)), key=lambda x:x[1], reverse=True)\n",
    "        answer=mmr(0.5,answer,3)\n",
    "        change=\"talk\"\n",
    "        way=\"Lex\"\n",
    "        abstway=0\n",
    "        \n",
    "        \n",
    "    elif(i_use in range(20,40)):\n",
    "        answer=sorted(list(lex_rank(text_use[i],len(text_use[i]),ikiti,ritu)), key=lambda x:x[1], reverse=True)\n",
    "        answer=mmr(0.5,answer,3)\n",
    "        change=\"emoji\"\n",
    "        way=\"Lex\"\n",
    "        abstway=1\n",
    "\n",
    "    elif(i_use in range(40,60)):\n",
    "        answer=sorted(list(lex_rank(text_use[i],len(text_use[i]),ikiti,ritu)), key=lambda x:x[1], reverse=True)\n",
    "        answer=mmr(0.5,answer,3)\n",
    "        change=\"keigo\"\n",
    "        way=\"Lex\"\n",
    "        abstway=2\n",
    "\n",
    "    elif(i_use in range(60,80)):\n",
    "        answer=sorted(list(topic_rank(i,len(text_use[i]),ikiti_topic,rate,text_use[i])), key=lambda x:x[1], reverse=True)\n",
    "        answer=mmr(0.5,answer,3)\n",
    "        change=\"talk\"\n",
    "        way=\"Topic\"\n",
    "        abstway=3\n",
    "\n",
    "    elif(i_use in range(80,100)):\n",
    "        answer=sorted(list(topic_rank(i,len(text_use[i]),ikiti_topic,rate,text_use[i])), key=lambda x:x[1], reverse=True)\n",
    "        answer=mmr(0.5,answer,3)\n",
    "        change=\"emoji\"\n",
    "        way=\"Topic\"\n",
    "        abstway=4\n",
    "\n",
    "    elif(i_use in range(100,120)):\n",
    "        answer=sorted(list(topic_rank(i,len(text_use[i]),ikiti_topic,rate,text_use[i])), key=lambda x:x[1], reverse=True)\n",
    "        answer=mmr(0.5,answer,3)\n",
    "        change=\"keigo\"\n",
    "        way=\"Topic\"\n",
    "        abstway=5\n",
    "\n",
    "    elif(i_use in range(120,140)):\n",
    "        answer=list(semanticVolume(text_use[i]))\n",
    "        change=\"talk\"\n",
    "        way=\"SV\"\n",
    "        abstway=6\n",
    "\n",
    "    elif(i_use in range(140,160)):\n",
    "        answer=list(semanticVolume(text_use[i]))\n",
    "        change=\"emoji\"\n",
    "        way=\"SV\"\n",
    "        abstway=7\n",
    "\n",
    "    elif(i_use in range(160,180)):\n",
    "        answer=list(semanticVolume(text_use[i]))\n",
    "        change=\"keigo\"\n",
    "        way=\"SV\"\n",
    "        abstway=8\n",
    "    \n",
    "    if(answer!=[]):\n",
    "        youyaku=[]\n",
    "        text_answer=\"\"\n",
    "        for t in range(3):\n",
    "            #print(answer[i][0])\n",
    "            youyaku.append(conversion(answer[t][0],change))\n",
    "            #print(youyaku)\n",
    "            #print(texts_title[872])\n",
    "        title=conversion(texts_title[i],change)\n",
    "        text_answer = text_answer+str(abstway)+\"\\n\"+title+\"\\n\"\n",
    "        text_answer=text_answer+\"\\n\".join(youyaku)+\"\\n\"\n",
    "        text_answer=text_answer+\"\".join(texts_original[i])\n",
    "        with open(str(shuffle[i2])+\"/\"+way+\"_\"+change+str(re.sub(r'[\\\\|/|:|?|.|\"|<|>|\\|]', '-', texts_title[i]))+\".txt\", \"w\") as f:  # w:上書きモード\n",
    "            f.write(text_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
